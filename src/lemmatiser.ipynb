{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a054ac90-a42b-4cfb-9045-716640f19ab3",
   "metadata": {},
   "source": [
    "# Lemmatiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a68529f1-3212-4d90-a93a-301f547dbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrenchLefffLemmatizer à installer si absent de l'env : \n",
    "# !pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git\n",
    "\n",
    "# Télécharger packages si absents : \n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf004ed7-80ac-4223-8e24-34d1128910ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1391353-b837-4004-9866-c19422257fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatiseur français\n",
    "lemmatizer_fr = FrenchLefffLemmatizer()\n",
    "# Lemmatiseur anglais\n",
    "lemmatizer_en = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123c9a0-fc63-48cf-a2ce-e81bbd9374fd",
   "metadata": {},
   "source": [
    "# Lecture des corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bb5b5aa-73e2-4450-aef6-b9c57406fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins corpus\n",
    "path_emea = Path(\"../data/Emea\")\n",
    "path_europarl = Path(\"../data/Europarl/\")\n",
    "\n",
    "# Création dico pour save corpus avec leurs noms de fichiers\n",
    "corpus_list = defaultdict(list)\n",
    "\n",
    "# Parcourir chaque fichier du dossier EMEA ou des sous-dossiers\n",
    "for file in path_emea.glob('**/*tok.true.clean*'):\n",
    "    with open(file, 'r') as file:\n",
    "        f1 = file.read()\n",
    "        # Extraire seulement le nom de fichiers comme clés \n",
    "        corpus_list[file.name.split('/')[-1]] = f1.split('\\n')\n",
    "\n",
    "# Parcourir chaque fichier du dossier Europarl ou des sous-dossiers\n",
    "for file in path_europarl.glob('**/*tok.true.clean*'):\n",
    "    with open(file, 'r') as file:\n",
    "        f2 = file.read()\n",
    "        # Extraire seulement le nom de fichiers comme clés \n",
    "        corpus_list[file.name.split('/')[-1]] = f2.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a5614-c2ba-4575-abec-b87450e94a31",
   "metadata": {},
   "source": [
    "# Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6592cdd-35b2-4618-a09f-de624860792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lemmatiser un texte\n",
    "def lemma_text(text: str, lang: str) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatisation d'un texte dans la langue spécifiée.\n",
    "\n",
    "    Cette fonction lemmatise en utilisant NLTK pour l'anglais et le français.\n",
    "    Français : lemmatisation de base effectuée avec lemmatizer_fr de FrenchLefffLemmatizer. \n",
    "    Anglais : POS tags utilisés pour lemmatisation plus précise. \n",
    "    \n",
    "    Args:\n",
    "        text (str): Texte à lemmatiser.\n",
    "        lang (str): Langue du texte. En format code de langue ('en' et 'fr')\n",
    "\n",
    "    Returns:\n",
    "        str: Texte lemmatisé.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si la langue renseignée n'est ni 'en' ni 'fr'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lemmatisation de chaque mot avec son POS tag\n",
    "    lem_words = []\n",
    "\n",
    "    if lang == \"fr\":\n",
    "        \n",
    "        # Tokeniser texte en mots\n",
    "        words = nltk.word_tokenize(text)\n",
    "            \n",
    "        # Lemmatisation du mot avec son POS tag\n",
    "        lem_words = [lemmatizer_fr.lemmatize(word) for word in words]\n",
    "        \n",
    "\n",
    "    elif lang == \"en\":\n",
    "        \n",
    "        # Tokeniser texte en mots\n",
    "        words = nltk.word_tokenize(text)\n",
    "    \n",
    "        # Obtenir POS tags des mots\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "        \n",
    "        for word, pos_tag in pos_tags:\n",
    "            # Convertir POS tags Penn Treebank en format WordNet\n",
    "            pos_tag = pos_tag[0].lower()\n",
    "            pos_tag = pos_tag if pos_tag in ['a', 'n', 'v'] else 'n'  # Convertir les POS tags inconnus en 'n' (nom)\n",
    "    \n",
    "            \n",
    "            # Lemmatisation du mot avec son POS tag\n",
    "            lemma = lemmatizer_en.lemmatize(word, pos=pos_tag)\n",
    "            lem_words.append(lemma)\n",
    "\n",
    "        \n",
    "    # Jointure des mots lemmatisés en une seule chaîne de caractères\n",
    "    lem_text = ' '.join(lem_words)\n",
    "    \n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e97fbf57-6a4f-4dee-b7d3-3a40c650cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création dico pour stocker phrases lemmatisées\n",
    "lem_corpus = defaultdict(list)\n",
    "\n",
    "# Parcourir chaque corpus\n",
    "for i, j in corpus_list.items():\n",
    "    # Création liste pour stocker phrases lemmatisées\n",
    "    lem_sents: list = []\n",
    "\n",
    "    # Pour français\n",
    "    if i.endswith('fr'):\n",
    "        # Lemmatiser chaque phrase\n",
    "        for sent in j:\n",
    "            # Lemmatiser phrase en français\n",
    "            lem_sent = lemma_text(sent, 'fr')\n",
    "            # Ajouter phrase lemmatisée à la liste\n",
    "            lem_sents.append(lem_sent)\n",
    "        # Stocker liste de phrases lemmatisées dans dico\n",
    "        lem_corpus[i] = lem_sents\n",
    "\n",
    "    # Pour anglais\n",
    "    elif i.endswith('en'):\n",
    "        # Lemmatiser chaque phrase\n",
    "        for sent in j:\n",
    "            # Lemmatiser phrase en anglais\n",
    "            lem_sent = lemma_text(sent, 'en')\n",
    "            # Ajouter phrase lemmatisée à la liste\n",
    "            lem_sents.append(lem_sent)\n",
    "        # Stocker liste de phrases lemmatisées dans dico\n",
    "        lem_corpus[i] = lem_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8bc9ff6f-10ed-4b45-8b22-a2565a12367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter les corpus lemmatisés\n",
    "for i, j in lem_corpus.items():\n",
    "    with open(f'../data/lemmatised/lemma_{i}', 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a8412-a54b-4546-9145-21757cd0a04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
